name: Run NanoCog Tests and Server

"on":
  push:
    branches: [ main, master ]
    branches-ignore: 
      - 'feature/**'
      - 'experimental/**'
      - 'temp/**'
      - 'wip/**'
    paths:
      - 'NanoCog/**'
      - '.github/workflows/ncrun.yml'
  pull_request:
    branches: [ main, master ]
    paths:
      - 'NanoCog/**'
      - '.github/workflows/ncrun.yml'
  workflow_run:
    workflows: ["Train NanoCog Model"]
    types:
      - completed
  workflow_dispatch:
    inputs:
      deploy_server:
        description: 'Deploy server after tests'
        required: false
        default: false
        type: boolean
      port:
        description: 'Server port (must be 1024-65535)'
        required: false
        default: '8080'
        type: string
      model_artifact:
        description: 'Model artifact name (leave empty for latest)'
        required: false
        default: ''
        type: string
      mock_atomspace:
        description: 'Use mock AtomSpace data'
        required: false
        default: true
        type: boolean

jobs:
  # === EMERGENT COGNITIVE PATTERNS (Jobs) ===
  # Multi-dimensional testing across Python versions for cognitive compatibility
  test:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        # Expanded matrix for broader cognitive framework compatibility
        python-version: ["3.8", "3.9", "3.10"]

    steps:
    # === ADAPTIVE ATTENTION ALLOCATION (Input Validation) ===
    - name: Validate port input
      if: ${{ github.event.inputs.port != '' }}
      run: |
        port="${{ github.event.inputs.port }}"
        if ! [[ "$port" =~ ^[0-9]+$ ]] || [ "$port" -lt 1024 ] || [ "$port" -gt 65535 ]; then
          echo "‚ùå Invalid port: $port. Must be numeric between 1024-65535"
          exit 1
        fi
        echo "‚úÖ Port validation passed: $port"
    
    - name: Checkout opencog-central
      uses: actions/checkout@v4
      with:
        path: opencog-central

    - name: Checkout nanoGPT
      uses: actions/checkout@v4
      with:
        repository: drzo/nanoGPT
        path: nanoGPT

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    # === EMERGENT COGNITIVE PATTERNS (Dependency Caching) ===
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/requirements*.txt', '**/setup.py', '**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install torch numpy tiktoken transformers requests fastapi uvicorn rich pytest pytest-asyncio httpx pytest-cov

    - name: Determine model artifact name
      id: model
      run: |
        if [[ -n "${{ github.event.inputs.model_artifact }}" ]]; then
          echo "artifact_name=${{ github.event.inputs.model_artifact }}" >> $GITHUB_OUTPUT
        else
          echo "artifact_name=nanocog-model-out-nanocog-ci" >> $GITHUB_OUTPUT
        fi

    # === HYPERGRAPH PATTERN ENCODING (Artifacts with Integrity) ===
    - name: Download model artifact (with retry logic)
      uses: nick-fields/retry@v2
      with:
        timeout_minutes: 10
        max_attempts: 3
        retry_wait_seconds: 30
        command: |
          # Use dawidd6/action-download-artifact with built-in error handling
          echo "Attempting to download model artifact: ${{ steps.model.outputs.artifact_name }}"
      continue-on-error: true
      id: download_attempt

    - name: Download model artifact
      uses: dawidd6/action-download-artifact@v2
      with:
        workflow: nctrain.yml
        name: ${{ steps.model.outputs.artifact_name }}
        path: nanoGPT/out-nanocog-ci
        if_no_artifact_found: warn
      continue-on-error: true
      id: artifact_download

    - name: Validate artifact integrity
      if: steps.artifact_download.outcome == 'success'
      run: |
        echo "üîç Validating downloaded artifact integrity..."
        if [ -f nanoGPT/out-nanocog-ci/ckpt.pt ]; then
          # Check file size (should be > 1KB for a real model)
          size=$(stat -f%z nanoGPT/out-nanocog-ci/ckpt.pt 2>/dev/null || stat -c%s nanoGPT/out-nanocog-ci/ckpt.pt 2>/dev/null || echo 0)
          if [ "$size" -gt 1024 ]; then
            echo "‚úÖ Artifact validation passed: checkpoint file exists and has reasonable size ($size bytes)"
            # Test if the file is a valid PyTorch checkpoint
            python3 -c "import torch; torch.load('nanoGPT/out-nanocog-ci/ckpt.pt', map_location='cpu'); print('‚úÖ PyTorch checkpoint validation passed')" || echo "‚ö†Ô∏è  Checkpoint validation warning"
          else
            echo "‚ùå Artifact validation failed: file too small ($size bytes)"
            rm -f nanoGPT/out-nanocog-ci/ckpt.pt
          fi
        else
          echo "‚ö†Ô∏è  No checkpoint file found in artifact"
        fi

    - name: Create model checkpoint if not found
      run: |
        if [ ! -f nanoGPT/out-nanocog-ci/ckpt.pt ]; then
          echo "No model artifact found, creating minimal checkpoint for testing"
          mkdir -p nanoGPT/out-nanocog-ci
          # Copy and run the minimal model creation script
          cp opencog-central/minimal_model_config.py nanoGPT/
          cd nanoGPT
          python minimal_model_config.py
          cd ..
        fi

    - name: Prepare directory structure
      run: |
        # Create necessary directories
        mkdir -p opencog-central/NanoCog/data
        mkdir -p opencog-central/NanoCog/tests
        mkdir -p opencog-central/NanoCog/introspection
        
        # Make sure nanoGPT can find the opencog-central repo
        ln -s $(pwd)/opencog-central $(pwd)/nanoGPT/opencog-central

    # === RECURSIVE IMPLEMENTATION PATHWAYS (Mock Data Modularization) ===
    - name: Test modular mock data generation
      run: |
        cd opencog-central
        
        # Test the new modular mock data system
        python3 -c "from NanoCog.mock import create_mock_cognitive_state; print('üß† Testing mock generation...'); data = create_mock_cognitive_state(); print('‚úÖ Mock data generated with', data['atom_count'], 'atoms')"
        python3 -c "from NanoCog.mock.cognitive_patterns import CognitivePatternGenerator; gen = CognitivePatternGenerator(); pattern = gen.generate_attention_pattern(10); print('‚úÖ Attention pattern generated with', len(pattern['attention_waves']), 'waves')"
        echo "üéØ Mock data modularization test completed successfully!"
        
        # Create a simple test directory structure for AtomSpace
        mkdir -p opencog-central/NanoCog/tests/mock_atomspace
        touch opencog-central/NanoCog/tests/mock_atomspace/__init__.py

    # === COGNITIVE SYNERGY OPTIMIZATION (Enhanced Testing) ===        
    - name: Create enhanced mock AtomSpace tests
      run: |
        # Create test directory structure
        mkdir -p opencog-central/NanoCog/tests/mock_atomspace
        touch opencog-central/NanoCog/tests/mock_atomspace/__init__.py
        
        # Copy the enhanced test file
        cp test_mock_enhanced.py opencog-central/NanoCog/tests/mock_atomspace/test_mock.py

    - name: Create test for introspection client
      run: |
        # Create test directory structure
        mkdir -p opencog-central/NanoCog/tests/introspection
        touch opencog-central/NanoCog/tests/introspection/__init__.py
        
        # Copy the test file
        cp test_introspection_client.py opencog-central/NanoCog/tests/introspection/test_atomspace_client.py

    - name: Create test for server
      run: |
        # Create test directory structure
        mkdir -p opencog-central/NanoCog/tests/server
        touch opencog-central/NanoCog/tests/server/__init__.py
        
        # Copy the test file
        cp test_server.py opencog-central/NanoCog/tests/server/test_server.py

    - name: Create test for CLI
      run: |
        # Create test directory structure
        mkdir -p opencog-central/NanoCog/tests/cli
        touch opencog-central/NanoCog/tests/cli/__init__.py
        
        # Copy the test file
        cp test_cli.py opencog-central/NanoCog/tests/cli/test_nctalk.py

    - name: Run unit tests for introspection client
      run: |
        cd opencog-central
        python -m pytest NanoCog/tests/introspection -v

    - name: Run mock AtomSpace tests
      if: ${{ github.event.inputs.mock_atomspace != 'false' }}
      run: |
        cd opencog-central
        python -m pytest NanoCog/tests/mock_atomspace -v

    # === COGNITIVE SYNERGY OPTIMIZATION (Pattern Recognition Tests) ===
    - name: Test cognitive pattern generation
      run: |
        cd opencog-central
        
        # Test cognitive pattern generation capabilities
        python3 -c "from NanoCog.mock.cognitive_patterns import CognitivePatternGenerator; gen = CognitivePatternGenerator(); print('üß† Testing cognitive patterns...'); pattern = gen.generate_attention_pattern(50); print('‚úÖ Attention pattern:', len(pattern['attention_waves']), 'time steps')"
        python3 -c "from NanoCog.mock.cognitive_patterns import CognitivePatternGenerator; gen = CognitivePatternGenerator(); pattern = gen.generate_learning_pattern(30); print('‚úÖ Learning pattern:', len(pattern['learning_phases']), 'phases')"
        python3 -c "from NanoCog.mock.cognitive_patterns import CognitivePatternGenerator; gen = CognitivePatternGenerator(); pattern = gen.generate_distributed_pattern(3); print('‚úÖ Distributed pattern:', len(pattern['synchronization_events']), 'sync events')"
        echo "üéØ Cognitive pattern generation tests completed successfully!"

    # TODO: Future recursive expansions - Placeholder test entries
    - name: Placeholder for advanced cognitive tests
      run: |
        echo "üîÆ Placeholder for future recursive cognitive pattern tests:"
        echo "  - Multi-agent interaction simulation tests"
        echo "  - Temporal cognitive state transition tests"  
        echo "  - Adaptive learning curve modeling tests"
        echo "  - Cross-modal pattern integration tests"
        echo "  - Hierarchical pattern emergence tests"
        echo "  - Neural-symbolic integration validation tests"
        echo "  - Distributed cognition robustness tests"
        echo "  - Emergent behavior prediction tests"

    - name: Run CLI tests
      run: |
        cd opencog-central
        python -m pytest NanoCog/tests/cli -v

    - name: Run server tests
      run: |
        cd opencog-central
        python -m pytest NanoCog/tests/server -v

    - name: Test NanoCog CLI with model
      run: |
        cd opencog-central
        # Copy and run the CLI test script
        cp ../test_nctalk_cli.py test_nctalk.py
        python test_nctalk.py

    # === DISTRIBUTED FRAMEWORK SYNERGY (Deployment with Health Monitoring) ===
    - name: Deploy server (if requested)
      if: ${{ github.event.inputs.deploy_server == 'true' }}
      run: |
        cd opencog-central
        
        # Start the server in the background with enhanced monitoring
        echo "üöÄ Starting NanoCog server with health monitoring..."
        python NanoCog/server.py --model_path=../nanoGPT/out-nanocog-ci/ckpt.pt --port=${{ github.event.inputs.port }} &
        SERVER_PID=$!
        echo "SERVER_PID=$SERVER_PID" >> $GITHUB_ENV
        
        # Enhanced startup verification with retries
        echo "‚è≥ Waiting for server startup with health checks..."
        for i in {1..10}; do
          sleep 3
          if curl -s --max-time 5 http://localhost:${{ github.event.inputs.port }}/status > /dev/null 2>&1; then
            echo "‚úÖ Server health check passed on attempt $i"
            break
          else
            echo "‚è≥ Health check attempt $i failed, retrying..."
            if [ $i -eq 10 ]; then
              echo "‚ùå Server failed to start after 10 attempts"
              kill $SERVER_PID 2>/dev/null || true
              exit 1
            fi
          fi
        done
        
        # Comprehensive health monitoring
        echo "üîç Running comprehensive server health diagnostics..."
        
        # Test status endpoint with detailed response validation
        echo "Testing /status endpoint..."
        STATUS_RESPONSE=$(curl -s --max-time 10 http://localhost:${{ github.event.inputs.port }}/status)
        echo "Status Response: $STATUS_RESPONSE"
        
        # Validate JSON response structure
        echo "$STATUS_RESPONSE" | jq . > /dev/null || {
          echo "‚ùå Invalid JSON response from /status endpoint"
          kill $SERVER_PID 2>/dev/null || true
          exit 1
        }
        
        # Test root endpoint
        echo "Testing / endpoint..."
        ROOT_RESPONSE=$(curl -s --max-time 10 http://localhost:${{ github.event.inputs.port }}/)
        echo "Root Response: $ROOT_RESPONSE"
        
        # Test cognitive chat endpoint with error handling
        echo "Testing /chat endpoint with cognitive query..."
        CHAT_RESPONSE=$(curl -s --max-time 15 -X POST http://localhost:${{ github.event.inputs.port }}/chat \
          -H "Content-Type: application/json" \
          -d '{
            "messages":[
              {"role":"user","content":"Explain distributed cognition synergy in CogPrime architecture"}
            ],
            "max_tokens":50,
            "temperature":0.7,
            "top_k":200,
            "stream":false
          }' || echo "CHAT_FAILED")
        
        if [ "$CHAT_RESPONSE" != "CHAT_FAILED" ]; then
          echo "Chat Response: $CHAT_RESPONSE"
          echo "‚úÖ Chat endpoint health check passed"
        else
          echo "‚ö†Ô∏è  Chat endpoint health check failed (non-critical)"
        fi
        
        # Monitor server health during testing period
        echo "üîÑ Monitoring server health for extended period..."
        MONITOR_DURATION=60
        HEALTH_CHECK_INTERVAL=10
        
        for ((t=0; t<MONITOR_DURATION; t+=HEALTH_CHECK_INTERVAL)); do
          sleep $HEALTH_CHECK_INTERVAL
          
          # Check if server process is still running
          if ! kill -0 $SERVER_PID 2>/dev/null; then
            echo "‚ùå Server process died unexpectedly at t=${t}s"
            exit 1
          fi
          
          # Health endpoint check
          if curl -s --max-time 5 http://localhost:${{ github.event.inputs.port }}/status > /dev/null 2>&1; then
            echo "‚úÖ Health check passed at t=${t}s"
          else
            echo "‚ö†Ô∏è  Health check failed at t=${t}s"
          fi
          
          # Memory and performance monitoring
          echo "üìä Server metrics at t=${t}s:"
          ps -p $SERVER_PID -o pid,ppid,cpu,pmem,etime,cmd 2>/dev/null || echo "Failed to get process stats"
        done
        
        echo "üéØ Server monitoring completed successfully"

    # === DISTRIBUTED FRAMEWORK SYNERGY (Enhanced Cleanup) ===
    - name: Cleanup server process
      if: always() && github.event.inputs.deploy_server == 'true'
      run: |
        echo "üßπ Performing enhanced server cleanup..."
        
        # Get SERVER_PID from environment or find by port
        if [ -n "$SERVER_PID" ]; then
          echo "Using stored SERVER_PID: $SERVER_PID"
        else
          echo "Searching for server process by port..."
          SERVER_PID=$(lsof -t -i:${{ github.event.inputs.port }} 2>/dev/null || echo "")
        fi
        
        if [ -n "$SERVER_PID" ] && [ "$SERVER_PID" != "" ]; then
          echo "üõë Attempting graceful shutdown of PID $SERVER_PID..."
          
          # Try graceful shutdown first
          kill -TERM $SERVER_PID 2>/dev/null || true
          
          # Wait for graceful shutdown
          for i in {1..10}; do
            if ! kill -0 $SERVER_PID 2>/dev/null; then
              echo "‚úÖ Server gracefully shut down after ${i}s"
              break
            fi
            sleep 1
            if [ $i -eq 10 ]; then
              echo "‚ö†Ô∏è  Graceful shutdown timeout, forcing termination..."
              kill -KILL $SERVER_PID 2>/dev/null || true
              sleep 2
              if ! kill -0 $SERVER_PID 2>/dev/null; then
                echo "‚úÖ Server force-terminated successfully"
              else
                echo "‚ùå Failed to terminate server process"
              fi
            fi
          done
        else
          echo "‚ÑπÔ∏è  No server process found to cleanup"
        fi
        
        # Additional cleanup: check for any remaining processes on the port
        REMAINING_PIDS=$(lsof -t -i:${{ github.event.inputs.port }} 2>/dev/null || echo "")
        if [ -n "$REMAINING_PIDS" ] && [ "$REMAINING_PIDS" != "" ]; then
          echo "üßπ Cleaning up remaining processes on port ${{ github.event.inputs.port }}: $REMAINING_PIDS"
          echo "$REMAINING_PIDS" | xargs -r kill -KILL 2>/dev/null || true
        fi
        
        # Verify port is clear
        sleep 2
        if lsof -i:${{ github.event.inputs.port }} 2>/dev/null; then
          echo "‚ö†Ô∏è  Port ${{ github.event.inputs.port }} still has active connections"
        else
          echo "‚úÖ Port ${{ github.event.inputs.port }} successfully cleared"
        fi
        
        echo "üéØ Enhanced cleanup completed"
